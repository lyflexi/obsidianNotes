non-blocking io 非阻塞 IO，NIO的三大组件是：
- Channel
- Buffer
- Selector
# Channel & Buffer

channel 有一点类似于 stream，它就是读写数据的==双向交互通道==，可以从 channel 将数据读入 buffer，也可以将 buffer 的数据写入 channel，相比于Java老版本的IOStream（字节流/字符流/文件流，要么是仅输入要么是仅输出），NIO中的channel是双向的并且更为底层
![[1 1.png]]
常见的 Channel 有
- FileChannel：处理文件（不支持非阻塞模式，了解即可）
- DatagramChannel：处理UDP，并且支持BIO非阻塞模式
- SocketChannel：处理TCP，并且支持BIO非阻塞模式
- ServerSocketChannel：处理TCP，并且支持BIO非阻塞模式

buffer 则用来缓冲读写数据，常见的 buffer 有
- ByteBuffer
    - MappedByteBuffer
    - DirectByteBuffer
    - HeapByteBuffer
- ShortBuffer
- IntBuffer
- LongBuffer
- FloatBuffer
- DoubleBuffer
- CharBuffer
# ByteBuffer 内存结构
使用ByteBuffer的正确姿势
1. 向 buffer 写入数据，例如调用 channel.read(buffer)或者buffer.put(byte b)
2. 调用 flip() 切换至**读模式**，然后调用 buffer.get() 从buffer中读取数据
3. 调用 clear() 或 compact() 切换至**写模式**
4. 重复 1~3 步骤
流程演示，初始状态给buffer分配10个字节的内存空间，Bytebuffer buf = ByteBuffer.allocate(10);
![[0021.png]]
调用 channel.read(buffer)，下图表示写入了 4 个字节后的状态，==position 是写入后的位置永远指向末位元素的下一个位置==，limit为写入限制10
![[0018.png]]
flip 动作发生后，position 切换为读取位置，limit切换为读取限制4
![[0019.png]]
调用 buffer.get()四次，从头开始读取 4 个字节后的buffer状态
![[0020.png]]
clear 动作发生后，状态如下，同时buffer被切换至写模式
![[0021 1.png]]
compact 方法，是把读取完的部分清空，未读完的部分向前移动到头，然后切换至写模式
要注意的是，老的cd没有被清除，而是原地保留了一份，但是这并不影响什么。因为Compact操作并没有移动Position指针，后续的写操作会覆盖掉老的cd，一举两得
![[Pasted image 20240202212445.png]]
# 粘包与半包处理
网络上经常是多条数据一起发送给服务端，假设数据之间使用`\n`进行分隔 ，但由于某种原因这些数据在接收时，被进行了重新组合，例如原始数据有3条为  `Hello,world\n             I'm zhangsan\n             How are you?\n`，服务器接收到的数据状况如下：
- 粘包：`Hello,world\nI'm zhangsan\nHo`
- 半包：`w are you?\`
```java
package nio.c2;  
  
import java.nio.ByteBuffer;  
  
import static nio.c2.ByteBufferUtil.debugAll;  
  
  
public class TestByteBufferExam {  
    public static void main(String[] args) {  
         /*  
         网络上有多条数据发送给服务端，数据之间使用 \n 进行分隔  
         但由于某种原因这些数据在接收时，被进行了重新组合，例如原始数据有3条为  
             Hello,world\n             I'm zhangsan\n             How are you?\n         
	     变成了下面的两个 byteBuffer (黏包，半包)  
             Hello,world\nI'm zhangsan\nHo             w are you?\n         
	     现在要求你编写程序，将错乱的数据恢复成原始的按 \n 分隔的数据  
         */        
        ByteBuffer source = ByteBuffer.allocate(32);  
        source.put("Hello,world\nI'm zhangsan\nHo".getBytes());  
        split(source);  
        source.put("w are you?\n".getBytes());  
        split(source);  
    }  
  
    private static void split(ByteBuffer source) {  
        source.flip();  
        for (int i = 0; i < source.limit(); i++) {  
            // 找到一条完整消息，get(i)不会导致position指针的移动  
            if (source.get(i) == '\n') {  
                int length = i + 1 - source.position();  
                // 把这条完整消息存入新的 ByteBuffer                
                ByteBuffer target = ByteBuffer.allocate(length);  
                // 从 source 读，向 target 写  
                for (int j = 0; j < length; j++) {  
                    //这里的get()会导致position指针的移动，这样下一个buffer就会读到后续的字节  
                    target.put(source.get());  
                }  
                debugAll(target);  
            }  
        }  
        source.compact();  
    }  
}
```
# Selector

selector 单从字面意思不好理解，需要结合服务器的设计演化来理解它的用途
## 服务器单线程版阻塞设计
目前是服务器端是阻塞IO模式
- connect - 客户端像服务器建立连接
- write -  客户端像服务器发送数据
- accept - 服务器端接受客户端连接，如果客户端当前没有发起连接则服务器的accept()方法阻塞- ServerSocketChannel.accept
- read -  服务器端读入客户端数据，如果客户端当前没有发送数据则服务器的read()方法阻塞- SocketChannel.read
服务器端设计如下，Server
服务器正常启动
```java
package nio.c4.bio;  
  
import lombok.extern.slf4j.Slf4j;  
  
import java.net.InetSocketAddress;  
import java.nio.ByteBuffer;  
import java.nio.channels.ServerSocketChannel;  
import java.nio.channels.SocketChannel;  
import java.util.ArrayList;  
import java.util.List;  
  
import static nio.c2.ByteBufferUtil.debugRead;  
  
/**  
 * @Author: ly  
 * @Date: 2024/2/4 12:50  
 */@Slf4j  
public class Server {  
    public static void main(String[] args) throws Exception {  
        // 使用 nio 来理解阻塞模式, 单线程  
		// 0. ByteBuffer  
        ByteBuffer buffer = ByteBuffer.allocate(16);  
        // 1. 创建了服务器  
        ServerSocketChannel ssc = ServerSocketChannel.open();  
  
		// 2. 绑定监听端口  
        ssc.bind(new InetSocketAddress(8080));  
  
        // 3. 连接集合  
        List<SocketChannel> channels = new ArrayList<>();  
        while (true) {  
            // 4. accept 建立与客户端连接， SocketChannel 用来与客户端之间通信  
            log.debug("connecting...");  
            SocketChannel sc = ssc.accept(); // 阻塞方法，线程停止运行  
            log.debug("connected... {}", sc);  
            channels.add(sc);  
            for (SocketChannel channel : channels) {  
                // 5. 接收客户端发送的数据  
                log.debug("before read... {}", channel);  
                channel.read(buffer); // 阻塞方法，线程停止运行  
                buffer.flip();  
                debugRead(buffer);  
                buffer.clear();  
                log.debug("after read...{}", channel);  
            }  
        }  
    }  
  
}
```
客户端设计如下，Client:
客户端通过Debug模式启动，在System.out.println("waiting...")处打上断点，这样同样一份客户端代码在调试期间支持通过Evaluate自定义发送数据，sc.write(Charset.defaultCharset().encode("client1"));
sc.write(Charset.defaultCharset().encode("client2"));
并且通过Edit-Allow multiple instances运行多个Client，模拟多个客户端连接请求
```java
package nio.c4.bio;  
  
import java.net.InetSocketAddress;  
import java.nio.channels.SocketChannel;  
import java.nio.charset.Charset;  
/**  
 * @Author: ly  
 * @Date: 2024/2/4 12:54  
 */public class Client {  
    // Debug模式运行时自定义发送数据，sc.write(Charset.defaultCharset().encode("client1"));  
    // 并且通过Edit-Allow multiple instances运行多个Client，模拟多个客户端连接请求  
    public static void main(String[] args) throws Exception{  
        SocketChannel sc = SocketChannel.open();  
        sc.connect(new InetSocketAddress("localhost", 8080));  
        System.out.println("waiting...");  
    }  
  
}
```
服务器端的打印信息如下，
![[Pasted image 20240204132837.png]]

## 服务器多线程版阻塞设计
多线程版缺点：
- 内存占用高，每开一个线程都是需要消耗内存的，比如开一个线程需要1M，那么开1000个线程就需要1G内存，如果连接数过多，必然导致 OOM
- 线程上下文切换成本高，线程的最终执行需要依靠CPU，当CPU的性能不足以同时处理比如1前个线程的时候，其余线程就会等待执行，这就是线程的上下文切换成本
因此服务器多线程版设计只适用于连接数比较少的场景
![[1.png]]
## 服务器线程池版阻塞设计
线程池版缺点，虽然线程池中的线程实现了复用，但是在当前socket连接断开之前，即使该socket连接没有任何读写请求，线程也不能去处理其他socket，线程仅能处理当前者一个socket 连接。
![[1 2.png]]
所以线程池治标不治本，如果有很多连接建立，但长时间 inactive，会阻塞线程池中所有线程，因此不适合长连接，只适合短连接

根本原因在于以上线程的工作模式仍然是阻塞模式
## 服务器单线程非阻塞模式设计
目前进化到了单线程非阻塞IO模式
- ssc.configureBlocking(false)，服务器的accept()方法不再阻塞会返回null继续运行-ServerSocketChannel.accept
- sc.configureBlocking(false)，服务器的read()方法不再阻塞会返回0但线程不必阻塞-SocketChannel.read
- 写数据时，线程只是等待数据写入 Channel 即可，无需等 Channel 通过网络把数据发送出去
但非阻塞模式下，即使没有连接建立，和可读数据，线程仍然在不断运行，白白浪费了 cpu
服务器端程序如下：
```java
package nio.c4.nio;  
  
import lombok.extern.slf4j.Slf4j;  
  
import java.net.InetSocketAddress;  
import java.nio.ByteBuffer;  
import java.nio.channels.ServerSocketChannel;  
import java.nio.channels.SocketChannel;  
import java.util.ArrayList;  
import java.util.List;  
  
import static nio.c2.ByteBufferUtil.debugRead;  
  
/**  
 * @Author: ly  
 * @Date: 2024/2/4 13:31  
 */@Slf4j  
public class Server {  
    public static void main(String[] args) throws Exception {  
  
  
// 使用 nio 来理解非阻塞模式, 单线程  
// 0. ByteBuffer  
        ByteBuffer buffer = ByteBuffer.allocate(16);  
// 1. 创建了服务器  
        ServerSocketChannel ssc = ServerSocketChannel.open();  
        ssc.configureBlocking(false); // 非阻塞模式,让accept方法不再阻塞  
// 2. 绑定监听端口  
        ssc.bind(new InetSocketAddress(8080));  
// 3. 连接集合  
        List<SocketChannel> channels = new ArrayList<>();  
        while (true) {  
            // 4. accept 建立与客户端连接， SocketChannel 用来与客户端之间通信  
            SocketChannel sc = ssc.accept(); // 非阻塞，线程还会继续运行，如果没有连接建立，但sc是null  
            if (sc != null) {  
                log.debug("connected... {}", sc);  
                sc.configureBlocking(false); // 非阻塞模式，让read方法不再阻塞  
                channels.add(sc);  
            }  
            for (SocketChannel channel : channels) {  
                // 5. 接收客户端发送的数据  
                int read = channel.read(buffer);// 非阻塞，线程仍然会继续运行，如果没有读到数据，read 返回 0  
                if (read > 0) {  
                    buffer.flip();  
                    debugRead(buffer);  
                    buffer.clear();  
                    log.debug("after read...{}", channel);  
                }  
            }  
        }  
    }  
}
```
客户端方程序如下，客户端不需要变化：
```java
package nio.c4.nio;  
  
import java.net.InetSocketAddress;  
import java.nio.channels.SocketChannel;  
import java.nio.charset.Charset;  
/**  
 * @Author: ly  
 * @Date: 2024/2/4 12:54  
 */public class Client {  
    // Debug模式运行时自定义发送数据，sc.write(Charset.defaultCharset().encode("client1"));  
    // 并且通过Edit-Allow multiple instances运行多个Client，模拟多个客户端连接请求  
    public static void main(String[] args) throws Exception{  
        SocketChannel sc = SocketChannel.open();  
        sc.connect(new InetSocketAddress("localhost", 8080));  
        System.out.println("waiting...");  
    }  
  
}
```
调试方式还是一样，客户端通过Debug模式启动，在System.out.println("waiting...")处打上断点，这样同样一份客户端代码在调试期间支持通过Evaluate自定义发送数据，sc.write(Charset.defaultCharset().encode("client1"));
sc.write(Charset.defaultCharset().encode("client2"));
并且通过Edit-Allow multiple instances运行多个Client，模拟多个客户端连接请求
1. 服务器的accept()方法不再阻塞了，观察现象：连续Debug启动两个客户端，服务器日志打印信息如下
```shell
13:34:01.420 [main] DEBUG nio.c4.nio.Server - connected... java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:64358]
13:34:32.663 [main] DEBUG nio.c4.nio.Server - connected... java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:64362]
```
2. 服务器的read()方法不再阻塞，服务端的client1和client2数据是连续打印出来了
![[Pasted image 20240204133900.png]]
但非阻塞模式下，即使没有新的连接建立，和可读数据产生，服务器线程仍然在不断运行，白白浪费了 cpu
## 服务器单线程selector版设计
使用服务器单线程while的方式导致线程大部分时间都在做无用功，而单线程可以配合 Selector 完成对多个 Channel 可读写事件的监控，这称之为多路复用。同样的，使用了selector的channel也是工作在非阻塞模式下，不会让线程吊死在一个 channel 上，不阻塞其他客户端的正常操作
- 有可连接事件时才去连接（非阻塞）
- 有可读事件才去读取（非阻塞）
- 有可写事件才去写入（非阻塞）
![[1 3.png]]
==初始化Selector选择器的相关操作，主要是将ServerSocketChannel绑定到selector上==
```java
// 1.创建 selector, 管理多个 channel
Selector selector = Selector.open();  
// 2.创建根Channel
ServerSocketChannel ssc = ServerSocketChannel.open();  
ssc.configureBlocking(false);
// 3.注册根Channel  
SelectionKey sscKey = ssc.register(selector, 0, null);  
// 4.selector只关注根Channel的accept事件 
sscKey.interestOps(SelectionKey.OP_ACCEPT);  
// 5.服务器绑定端口
ssc.bind(new InetSocketAddress(8080));
```
ServerSocketChannel初始化成功之后，==单线程进入while循环一开始就要调用selector.select()方法进行事件监听，这一步是阻塞的，避免了CPU空转（基于水平触发，当感兴趣的事件到来，该方法会反复被触发，即放行）==
```java
while (true) {  
	//方法1，selector.select()
	int count = selector.select();  
	//方法2，阻塞直到绑定事件发生，或是超时（时间单位为 ms）
	//int count = selector.select(long timeout);
	//方法3，不会阻塞，也就是不管有没有事件，立刻返回，自己根据返回值检查是否有事件
	//int count = selector.selectNow();
	......
}
```
### 处理accept事件，必须手动iter.remove()
这是因为 nio 底层默认使用的是水平触发（反复触发），因此用过一次事件必须执行iter.remove()将key移除，否则其他事件来的时候，老的key依然在迭代器当中容易出现空指针异常。原因如下因为 select 在事件发生后，就会将相关的 key 放入 selectedKeys 集合，但不会在处理完后从 selectedKeys 集合中自动清除，需要我们自己编码移除accept事件与SelectionKey的关联
- 第一次触发了 ssckey 上的 accept 事件，没有移除 ssckey
- 第二次触发了 sckey 上的 read 事件，但这时 selectedKeys 中还有上次的 ssckey ，在处理时因为没有真正的 serverSocket 连接，而是老的客户端连接写了数据，服务端读事件发生，再重新迭代的时候拿到了老的 ssckey发生空指针异常
![[Pasted image 20240204212809.png]]
服务端代码如下：
```java
package nio.c4.selector.accept;  
  
import lombok.extern.slf4j.Slf4j;  
  
import java.io.IOException;  
import java.net.InetSocketAddress;  
import java.nio.ByteBuffer;  
import java.nio.channels.SelectionKey;  
import java.nio.channels.Selector;  
import java.nio.channels.ServerSocketChannel;  
import java.nio.channels.SocketChannel;  
import java.nio.charset.Charset;  
import java.util.Iterator;  
import java.util.Set;  
  
import static nio.c2ByteBuffer.ByteBufferUtil.debugAll;  
  
@Slf4j  
public class Server {  
  
    public static void main(String[] args) throws IOException {  
        // 1. 创建 selector, 管理多个 channel        
        Selector selector = Selector.open();  
        ServerSocketChannel ssc = ServerSocketChannel.open();  
        ssc.configureBlocking(false);  
        // 2. 建立 selector 和 channel 的联系（注册）  
        // SelectionKey 就是将来事件发生后，通过它可以知道事件和哪个channel的事件  
        SelectionKey sscKey = ssc.register(selector, 0, null);  
        // key 只关注 accept 事件  
        sscKey.interestOps(SelectionKey.OP_ACCEPT);  
        log.debug("sscKey:{}", sscKey);  
        ssc.bind(new InetSocketAddress(8080));  
  
        while (true) {  
            int count = selector.select();  
            log.debug("select count: {}", count);  
  
            // 获取所有事件  
            Set<SelectionKey> keys = selector.selectedKeys();  
  
            // 遍历所有事件，逐一处理  
            Iterator<SelectionKey> iter = keys.iterator();  
            while (iter.hasNext()) {  
                SelectionKey key = iter.next();  
                // 用过一次事件必须将事件移除  
                iter.remove();  
                // 判断事件类型  
                if (key.isAcceptable()) {  
                    ServerSocketChannel c = (ServerSocketChannel) key.channel();  
                    // 必须处理  
                    SocketChannel sc = c.accept();  
                    log.debug("{}", sc);  
                }  
  
            }  
        }  
  
    }  
}
```
### 处理客户端关闭，必须手动key.cancel()
无论是客户端正常关闭sc.close()，还是客户端异常终止，都会发出read事件，会触发服务器的监听。因为 nio 底层使用的是水平触发（因为该事件没有被消费，服务器循环中该事件会无休止的触发），因此为了防止客户端关闭带来的read事件的水平触发，服务器必须执行cancel操作，意味着将该事件从Selector中移除后续不会再监听事件，并从 keys 集合中删除 key ，cancel操作与之前的remove操作对比如下
- `remove`方法针对的是`SelectionKey`集合，仅仅移除了keys集合中的key-read
- `cancel`方法针对的是`Selector`对象本身，从根本上取消了信道上注册的操作。这才是真正的删除事件操作，也就是说selector.select()再也监听不到可读事件了
```java
while (true) {  
    // 3. select 方法, 没有事件发生，线程阻塞，有事件，线程才会恢复运行  
    // 水平触发，select 在事件未处理时，它不会阻塞, 事件发生后要么处理，要么取消，不能置之不理  
    selector.select();  
    // 4. 处理事件, selectedKeys 内部包含了所有发生的事件  
    Iterator<SelectionKey> iter = selector.selectedKeys().iterator(); // accept, read  
    while (iter.hasNext()) {  
        SelectionKey key = iter.next();  
        // 处理key 时，要从 selectedKeys 集合中删除，否则下次处理就会有问题  
        iter.remove();  
        log.debug("key: {}", key);  
        // 5. 区分事件类型  
        if (key.isAcceptable()) { // 如果是 accept            
	        ServerSocketChannel channel = (ServerSocketChannel) key.channel();  
            SocketChannel sc = channel.accept();  
            sc.configureBlocking(false);  
  
            SelectionKey scKey = sc.register(selector, 0, null);  
            scKey.interestOps(SelectionKey.OP_READ);  
            log.debug("{}", sc);  
            log.debug("scKey:{}", scKey);  
        } else if (key.isReadable()) { // 如果是 read            
	        try {  
                SocketChannel channel = (SocketChannel) key.channel(); // 拿到触发事件的channel  
                ByteBuffer buffer = ByteBuffer.allocate(4);  
                int read = channel.read(buffer); // 如果是正常断开，read 的方法的返回值是 -1                
                if(read == -1) {  
                    key.cancel();  
                } else {  
                    buffer.flip();  
                    debugAll(buffer);  
                    System.out.println(Charset.defaultCharset().decode(buffer));  
                }  
            } catch (IOException e) {  
                e.printStackTrace();  
                key.cancel();  // 因为客户端断开了,因此需要将 key 取消（从 selector 的 keys 集合中真正删除 key）  
            }  
        }  
    }  
}
```
### 处理read事件，数据边界问题
数据边界问题就是半包问题，服务器ByteBuffer不足以全部接收客户端发来的数据时，会导致乱码现象：比如我设定服务器ByteBuffer容量为4个字节，客户端发来”中国“这两个汉字，默认UTF-8字符集一个汉字占据3个字节，因此”国“字就无法被正常读取
```shell
17:42:37.740 [main] DEBUG nio.c4.selector.read.Server - scKey:channel=java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:55323], selector=sun.nio.ch.WEPollSelectorImpl@3796751b, interestOps=1, readyOps=0
17:42:58.365 [main] DEBUG nio.c4.selector.read.Server - key: channel=java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:55323], selector=sun.nio.ch.WEPollSelectorImpl@3796751b, interestOps=1, readyOps=1
17:42:58.369 [main] DEBUG io.netty.util.internal.logging.InternalLoggerFactory - Using SLF4J as the default logging framework
+--------+-------------------- all ------------------------+----------------+
position: [0], limit: [4]
         +-------------------------------------------------+
         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |
+--------+-------------------------------------------------+----------------+
|00000000| e4 b8 ad e5                                     |....            |
+--------+-------------------------------------------------+----------------+
中�
17:42:58.378 [main] DEBUG nio.c4.selector.read.Server - key: channel=java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:55323], selector=sun.nio.ch.WEPollSelectorImpl@3796751b, interestOps=1, readyOps=1
+--------+-------------------- all ------------------------+----------------+
position: [0], limit: [2]
         +-------------------------------------------------+
         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |
+--------+-------------------------------------------------+----------------+
|00000000| 9b bd 00 00                                     |....            |
+--------+-------------------------------------------------+----------------+
��
```
以下的情况都会造成数据边界问题，很容易想到的解决方案是发送方固定数据长度，并且服务器按预定长度读取，如果数据不足一个buffer则进行填充对齐，但是有很大的缺点就是浪费存储与带宽
![[0023.png]]
Netty的解决方式如下：将传输的数据规定为TLV 格式，即 【Type 类型、Length 长度标志后续一节真实数据的长度、Value 数据】，如果一个ByteBuffer存不下那就对buffer进行扩容
- Http 1.1 是 TLV 格式
- Http 2.0 是 LTV 格式
这里我们自己实现另外一种思路，规定数据以分隔符`\n`进行分割，服务器按分隔符拆分读取，如果直到ByteBuffer装满还没有遇到分隔符，那咱也对buffer进行扩容。其中扩容的思路如下：
![[1 4.png]]

服务器程序修改如下：关键位置添加了注释
每个 channel 都需要记录可能被切分的消息，因为 ByteBuffer 不能被多个 channel 共同使用，因此需要为每个 channel 维护一个独立的 ByteBuffer，因此：
- 迭代器内每一个channel都各自初始化出来ByteBuffer
- 通过attachment将ByteBuffer绑定到不同的channel上
```java
package nio.c4.selector.read;  
  
import lombok.extern.slf4j.Slf4j;  
  
import java.io.IOException;  
import java.net.InetSocketAddress;  
import java.nio.ByteBuffer;  
import java.nio.channels.SelectionKey;  
import java.nio.channels.Selector;  
import java.nio.channels.ServerSocketChannel;  
import java.nio.channels.SocketChannel;  
import java.nio.charset.Charset;  
import java.util.Iterator;  
  
import static nio.c2ByteBuffer.ByteBufferUtil.debugAll;  
  
@Slf4j  
public class Server {  
  
    public static void main(String[] args) throws IOException {  
        Selector selector = Selector.open();  
        ServerSocketChannel ssc = ServerSocketChannel.open();  
        ssc.configureBlocking(false);  
        SelectionKey sscKey = ssc.register(selector, 0, null);  
        sscKey.interestOps(SelectionKey.OP_ACCEPT);  
        log.debug("sscKey:{}", sscKey);  
        ssc.bind(new InetSocketAddress(8080));  
        while (true) {  
            selector.select();  
            Iterator<SelectionKey> iter = selector.selectedKeys().iterator(); // accept, read  
            while (iter.hasNext()) {  
  
                SelectionKey key = iter.next();  
                iter.remove();  
                log.debug("key: {}", key);  
                if (key.isAcceptable()) { // 如果是 accept                    
                ServerSocketChannel channel = (ServerSocketChannel) key.channel();  
                    SocketChannel sc = channel.accept();  
                    sc.configureBlocking(false);  
                    ByteBuffer buffer = ByteBuffer.allocate(16); // attachment  
                    // 将一个 byteBuffer 作为附件关联到 selectionKey 上  
                    SelectionKey scKey = sc.register(selector, 0, buffer);  
                    scKey.interestOps(SelectionKey.OP_READ);  
                    log.debug("{}", sc);  
                    log.debug("scKey:{}", scKey);  
                } else if (key.isReadable()) {  
                    try {  
                        SocketChannel channel = (SocketChannel) key.channel();  
                        // 获取 selectionKey 上关联的附件  
                        ByteBuffer buffer = (ByteBuffer) key.attachment();  
                        int read = channel.read(buffer);  
                        if(read == -1) {  
                            key.cancel();  
                        } else {  
                            split(buffer);  
                            // 需要扩容  
                            if (buffer.position() == buffer.limit()) {  
                                ByteBuffer newBuffer = ByteBuffer.allocate(buffer.capacity() * 2);  
                                buffer.flip();  
                                newBuffer.put(buffer); // 0123456789abcdef3333\n  
                                key.attach(newBuffer);  
                            }  
                        }  
                    } catch (IOException e) {  
//                        e.printStackTrace();//只是打印异常信息，Server不会再异常中止了  
                        key.cancel();  
                    }  
                }  
            }  
        }  
    }  
    private static void split(ByteBuffer source) {  
        source.flip();  
        for (int i = 0; i < source.limit(); i++) {  
            // 找到一条完整消息  
            if (source.get(i) == '\n') {  
                int length = i + 1 - source.position();  
                // 把这条完整消息存入新的 ByteBuffer                
                ByteBuffer target = ByteBuffer.allocate(length);  
                // 从 source 读，向 target 写  
                for (int j = 0; j < length; j++) {  
                    target.put(source.get());  
                }  
                debugAll(target);  
            }  
        }  
        source.compact(); // 0123456789abcdef  position 16 limit 16  
    }  
}
```
客户端程序如下：
```java
package nio.c4.selector.read;  
  
import java.io.IOException;  
import java.net.InetSocketAddress;  
import java.net.SocketAddress;  
import java.nio.channels.SocketChannel;  
import java.nio.charset.Charset;  
  
public class Client {  
    public static void main(String[] args) throws IOException {  
        SocketChannel sc = SocketChannel.open();  
        sc.connect(new InetSocketAddress("localhost", 8080));  
        SocketAddress address = sc.getLocalAddress();  
        sc.write(Charset.defaultCharset().encode("123\n456\n123456789123456789\n"));//测试切分与扩容
        System.out.println("waiting...");  
//        sc.close();  
    }  
}
```
服务器打印信息如下：
```shell
20:11:09.357 [main] DEBUG nio.c4.selector.read.Server - key: channel=sun.nio.ch.ServerSocketChannelImpl[/[0:0:0:0:0:0:0:0]:8080], selector=sun.nio.ch.WEPollSelectorImpl@3796751b, interestOps=16, readyOps=16
20:11:09.357 [main] DEBUG nio.c4.selector.read.Server - java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:61235]
20:11:09.357 [main] DEBUG nio.c4.selector.read.Server - scKey:channel=java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:61235], selector=sun.nio.ch.WEPollSelectorImpl@3796751b, interestOps=1, readyOps=0
20:11:09.358 [main] DEBUG nio.c4.selector.read.Server - key: channel=java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:61235], selector=sun.nio.ch.WEPollSelectorImpl@3796751b, interestOps=1, readyOps=1
+--------+-------------------- all ------------------------+----------------+
position: [4], limit: [4]
         +-------------------------------------------------+
         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |
+--------+-------------------------------------------------+----------------+
|00000000| 31 32 33 0a                                     |123.            |
+--------+-------------------------------------------------+----------------+
+--------+-------------------- all ------------------------+----------------+
position: [4], limit: [4]
         +-------------------------------------------------+
         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |
+--------+-------------------------------------------------+----------------+
|00000000| 34 35 36 0a                                     |456.            |
+--------+-------------------------------------------------+----------------+
20:11:09.358 [main] DEBUG nio.c4.selector.read.Server - key: channel=java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:61235], selector=sun.nio.ch.WEPollSelectorImpl@3796751b, interestOps=1, readyOps=1
20:11:09.359 [main] DEBUG nio.c4.selector.read.Server - key: channel=java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:61235], selector=sun.nio.ch.WEPollSelectorImpl@3796751b, interestOps=1, readyOps=1
+--------+-------------------- all ------------------------+----------------+
position: [19], limit: [19]
         +-------------------------------------------------+
         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |
+--------+-------------------------------------------------+----------------+
|00000000| 31 32 33 34 35 36 37 38 39 31 32 33 34 35 36 37 |1234567891234567|
|00000010| 38 39 0a                                        |89.             |
+--------+-------------------------------------------------+----------------+
20:11:09.866 [main] DEBUG nio.c4.selector.read.Server - key: channel=java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:61235], selector=sun.nio.ch.WEPollSelectorImpl@3796751b, interestOps=1, readyOps=1

```
### 处理write事件，如何非阻塞写
服务器代码WriteServerBlock如下：模拟了50000000个字节的发送（向客户端写），但是限于网络传输能力，Channel 未必时时可写
```java
package nio.c4.selector.write;  
  
import java.io.IOException;  
import java.net.InetSocketAddress;  
import java.nio.ByteBuffer;  
import java.nio.channels.*;  
import java.nio.charset.Charset;  
import java.util.Iterator;  
  
public class WriteServerBlock {  
    public static void main(String[] args) throws IOException {  
        ServerSocketChannel ssc = ServerSocketChannel.open();  
        ssc.configureBlocking(false);  
        Selector selector = Selector.open();  
        ssc.register(selector, SelectionKey.OP_ACCEPT);  
        ssc.bind(new InetSocketAddress(8080));  
        while (true) {  
            selector.select();  
            Iterator<SelectionKey> iter = selector.selectedKeys().iterator();  
            while (iter.hasNext()) {  
                SelectionKey key = iter.next();  
                iter.remove();  
                if (key.isAcceptable()) {  
                    SocketChannel sc = ssc.accept();  
                    sc.configureBlocking(false);  
                    SelectionKey sckey = sc.register(selector, 0, null);  
                    sckey.interestOps(SelectionKey.OP_READ);  
                    // 1. 向客户端发送大量数据  
                    StringBuilder sb = new StringBuilder();  
                    for (int i = 0; i < 50000000; i++) {  
                        sb.append("a");  
                    }  
                    ByteBuffer buffer = Charset.defaultCharset().encode(sb.toString());  
                    while (buffer.hasRemaining()) {  
                        // 2. 返回值代表实际写入的字节数  
                        int write = sc.write(buffer);  
                        System.out.println(write);  
                    }  
                }  
            }  
        }  
    }  
}
```
客户端程序如下：WriteClient
```java
package nio.c4.selector.write;  
  
import java.io.IOException;  
import java.net.InetSocketAddress;  
import java.nio.ByteBuffer;  
import java.nio.channels.SocketChannel;  
  
public class WriteClient {  
    public static void main(String[] args) throws IOException {  
        SocketChannel sc = SocketChannel.open();  
        sc.connect(new InetSocketAddress("localhost", 8080));  
  
        // 3. 接收数据  
        int count = 0;  
        while (true) {  
            ByteBuffer buffer = ByteBuffer.allocate(1024 * 1024);  
            count += sc.read(buffer);  
            System.out.println(count);  
            buffer.clear();  
        }  
    }  
}
```
服务器信息打印，可以看到，网络的传输能力是有限的，因此50000000字节的数据分了很多次传输，0代表此次传输的过程中通道是满的暂时被阻塞了，因此这样的编程方式不符合NIO的理念，因此禁止使用while(buffer.hasRemaining())去发送数据
```shell
524284
4456414
0
6160337
0
0
0
7995331
0
0
0
0
4718556
0
2490349
0
0
2621420
0
2621420
0
0
0
2621420
0
0
0
2621420
0
0
0
2621420
0
0
0
0
0
2621420
0
0
0
0
0
0
2621420
0
0
2621420
0
0
0
0
0
0
0
2621420
0
0
0
0
0
0

61949

```
改进后的服务器端代码实现了非阻塞模式的发送（向客户端写）：
- 初始只写一次
- 如果初始的一次没有写完，则给当前通道绑定写事件，并调用attach方法将buffer附到当前的sckey上面
- selector.select()会循环监听可写事件，继续写剩余没有写完的数据
- 如果最后终于全部写完，记得注销这个可写事件，否则服务器只要向 channel 发送数据时，socket 缓冲可写，这个事件会频繁触发。因此应当只在 socket 缓冲区写不下时再关注可写事件，数据写完之后再取消关注。还有最后一点，将buffer附件从当前sckey上移除
```java
package nio.c4.selector.write;  
  
import java.io.IOException;  
import java.net.InetSocketAddress;  
import java.nio.ByteBuffer;  
import java.nio.channels.*;  
import java.nio.charset.Charset;  
import java.util.Iterator;  
  
public class WriteServer {  
    public static void main(String[] args) throws IOException {  
        ServerSocketChannel ssc = ServerSocketChannel.open();  
        ssc.configureBlocking(false);  
        Selector selector = Selector.open();  
        ssc.register(selector, SelectionKey.OP_ACCEPT);  
        ssc.bind(new InetSocketAddress(8080));  
        while (true) {  
            selector.select();  
            Iterator<SelectionKey> iter = selector.selectedKeys().iterator();  
            while (iter.hasNext()) {  
                SelectionKey key = iter.next();  
                iter.remove();  
                if (key.isAcceptable()) {  
	                //这里直接用ssc.accept()，等价于通过key.channel得到sc，用sc.accept()
                    SocketChannel sc = ssc.accept();  
                    sc.configureBlocking(false);  
                    SelectionKey sckey = sc.register(selector, 0, null);  
                    sckey.interestOps(SelectionKey.OP_READ);  
                    // 1. 向客户端发送大量数据  
                    StringBuilder sb = new StringBuilder();  
                    for (int i = 0; i < 5000000; i++) {  
                        sb.append("a");  
                    }  
                    ByteBuffer buffer = Charset.defaultCharset().encode(sb.toString());  
  
                    // 2. 返回值代表实际写入的字节数  
                    int write = sc.write(buffer);  
                    System.out.println(write);  
  
                    // 3. 判断是否有剩余内容  
                    if (buffer.hasRemaining()) {  
                        // 4. 关注可写事件   1                     4                        
                        sckey.interestOps(sckey.interestOps() + SelectionKey.OP_WRITE);  
//                        sckey.interestOps(sckey.interestOps() | SelectionKey.OP_WRITE);  
                        // 5. 把未写完的数据挂到 sckey 上  
                        sckey.attach(buffer);  
                    }  
                } else if (key.isWritable()) {  
                    ByteBuffer buffer = (ByteBuffer) key.attachment();  
                    SocketChannel sc = (SocketChannel) key.channel();  
                    int write = sc.write(buffer);  
                    System.out.println(write);  
                    // 6. 清理操作  
                    if (!buffer.hasRemaining()) {  
                        key.attach(null); // 需要清除buffer  
                        key.interestOps(key.interestOps() - SelectionKey.OP_WRITE);//不需关注可写事件  
                    }  
                }  
            }  
        }  
    }  
}
```
客户端端代码不变，客户端打印信息，总共收到5000000个字节，计数正确
```shell
131071
262142
393213
524284
655355
786426
917497
1048568
1179639
1310710
1441781
1572852
1703923
1834994
1966065
2097136
2228207
2359278
2490349
2621420
2752491
2883562
3014633
3145704
3276775
3407846
3538917
3669988
3801059
3932130
4063201
4194272
4325343
4456414
4587485
4718556
4784051
4915041
5000000

```

服务器打印信息，可以看到服务器确实是一次写不完，分了两次去写
```shell
4980698
19302
```


# 服务器多线程版非阻塞设计

设计架构图如下，Boos节点只负责响应accept事件，服务器对读事件的处理全部交给Worker节点来完成
![[Pasted image 20240205162436.png]]
服务器端代码如下，需要注意的几个点：
- boos节点和Worker节点拥有各自的任务线程，以及各自的Selector
- accept事件到来之后，boos任务负责创建新的SocketChannel，并调用worker的register方法
	- 单独创建worker任务线程
	- 提前通过selector.wakeup()唤醒worker任务的selector.select()方法，因为selector.select()位于worker线程中执行，sc.register(selector, SelectionKey.OP_READ, null)位于boos线程中执行，一旦worker任务的selector.select()先执行，selector.select()就会与sc.register(selector, SelectionKey.OP_READ, null)互相等待，形成死锁
	- 将读事件绑定到这个SocketChannel上，以便worker任务下次循环的时候放行selector.select()
- 通过Runtime.getRuntime().availableProcessors()得到处理器个数并初始化worker个数，搭配计数器AtomicInteger来简单实现一个worker节点的负载均衡
```java
package nio.c4;  
  
import lombok.extern.slf4j.Slf4j;  
  
import java.io.IOException;  
import java.net.InetSocketAddress;  
import java.nio.ByteBuffer;  
import java.nio.channels.*;  
import java.util.Iterator;  
import java.util.concurrent.ConcurrentLinkedQueue;  
import java.util.concurrent.atomic.AtomicInteger;  
  
import static nio.c2ByteBuffer.ByteBufferUtil.debugAll;  
  
@Slf4j  
public class MultiThreadServer {  
    public static void main(String[] args) throws IOException {  
        Thread.currentThread().setName("boss");  
        ServerSocketChannel ssc = ServerSocketChannel.open();  
        ssc.configureBlocking(false);  
        Selector boss = Selector.open();  
        SelectionKey bossKey = ssc.register(boss, 0, null);  
        bossKey.interestOps(SelectionKey.OP_ACCEPT);  
        ssc.bind(new InetSocketAddress(8080));  
        // 1. 创建固定数量的 worker 并初始化  
        Worker[] workers = new Worker[Runtime.getRuntime().availableProcessors()];  
        for (int i = 0; i < workers.length; i++) {  
            workers[i] = new Worker("worker-" + i);  
        }  
        AtomicInteger index = new AtomicInteger();  
        while(true) {  
            boss.select();  
            Iterator<SelectionKey> iter = boss.selectedKeys().iterator();  
            while (iter.hasNext()) {  
                SelectionKey key = iter.next();  
                iter.remove();  
                if (key.isAcceptable()) {  
                    SocketChannel sc = ssc.accept();  
                    sc.configureBlocking(false);  
                    log.debug("connected...{}", sc.getRemoteAddress());  
                    // 2. 关联 selector                    
                    log.debug("before register...{}", sc.getRemoteAddress());  
                    // round robin 轮询  
                    workers[index.getAndIncrement() % workers.length].register(sc); // boss 调用 初始化 selector , 启动 worker-0  
                    log.debug("after register...{}", sc.getRemoteAddress());  
                }  
            }  
        }  
    }  
    static class Worker implements Runnable{  
        private Thread thread;  
        private Selector selector;  
        private String name;  
        private volatile boolean start = false; // 还未初始化  
        private ConcurrentLinkedQueue<Runnable> queue = new ConcurrentLinkedQueue<>();  
        public Worker(String name) {  
            this.name = name;  
        }  
  
        // 初始化线程，和 selector        
        public void register(SocketChannel sc) throws IOException {  
            if(!start) {  
                selector = Selector.open();  
                thread = new Thread(this, name);  
                thread.start();  
                start = true;  
            }  
            selector.wakeup(); // 唤醒 select 方法 boss            
            sc.register(selector, SelectionKey.OP_READ, null); // boss  
        }  
  
        /*start()方法是多线程编程的关键，JVM会创建一个新的线程，并在这个新线程中调用run()方法。而run()方法则是普通的方法调用*/  
        @Override  
        public void run() {  
            while(true) {  
                try {  
                    selector.select(); // 如果接收不到可读事件，则worker-0阻塞  
                    Iterator<SelectionKey> iter = selector.selectedKeys().iterator();  
                    while (iter.hasNext()) {  
                        SelectionKey key = iter.next();  
                        iter.remove();  
                        if (key.isReadable()) {  
                            ByteBuffer buffer = ByteBuffer.allocate(16);  
                            SocketChannel channel = (SocketChannel) key.channel();  
                            log.debug("read...{}", channel.getRemoteAddress());  
                            channel.read(buffer);  
                            buffer.flip();  
                            debugAll(buffer);  
                        }  
                    }  
                } catch (IOException e) {  
                    e.printStackTrace();  
                }  
            }  
        }  
    }  
}
```