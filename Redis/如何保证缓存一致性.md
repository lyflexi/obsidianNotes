引入缓存之后，你就会面临一个问题：之前数据只存在数据库中，现在要放到缓存中，具体要怎么存呢？
最简单直接的方案是「全量数据刷到缓存中」：
- 写请求只更新数据库，不更新缓存。我们只是启动一个定时任务，定时把数据库的数据，更新到缓存中
- 所有读请求都可以直接「命中」缓存，不需要再查数据库，性能非常高。
但缺点也很明显，数据不一致：因为是「定时」刷新缓存，缓存和数据库存在不一致（取决于定时任务的执行频率）

要想保证缓存和数据库「实时」一致，那就不能再用定时任务将数据库数据刷新进缓存了。
所以，当数据发生更新时，我们不仅要操作数据库，还要一并操作缓存。具体操作就是：

- 方案一：双写模式：修改一条数据时，先更新数据库，之后更新缓存
- 方案二：失效模式：修改一条数据时，先更新数据库，之后删除缓存。
- 方案三：延迟双删：在方案二的基础之上，写线程A隔一段时间再一次删除缓存，防止缓存被读线程B种回旧值

# 双写模式：写操作种回旧值

假设我们采用「先更新数据库，再更新缓存」的方案，并且两步都可以「成功执行」的前提下，如果存在并发，情况会是怎样的呢？

有线程 A 和线程 B 两个线程，需要更新「同一条」数据，会发生这样的场景
1. 线程 A 更新数据库（X = 1）
2. 线程 B 更新数据库（X = 2）
3. 线程 B 更新缓存（X = 2）
4. 线程 A 更新缓存（X = 1）

最终 X 的值在缓存中是 1，在数据库中是 2，发生不一致。
![[Pasted image 20240120193106.png]]
除此之外，我们从「缓存利用率」的角度来评估这个方案，也是不太推荐的。
- 这是因为每次数据发生变更，都「无脑」更新缓存，但是缓存中的数据不一定会被「马上读取」，这就会导致缓存中可能存放了很多不常访问的数据，浪费缓存资源。
- 而且很多情况下，写到缓存中的值，并不是与数据库中的值一一对应的，很有可能是先查询数据库，再经过一系列「计算」得出一个值，才把这个值才写到缓存中。
由此可见，这种「更新数据库 + 更新缓存」的方案，不仅缓存利用率不高，还会造成机器性能的浪费。

# 失效模式：读操作种回旧值
此种方案默认有个前提是，只有读数据库默认会写缓存，这次来了三个线程：
- 一个写线程，线程A
- 一个写线程，线程B
- 一个读线程，线程C

概念澄清：
- 旧DB：线程A写的DB
- 新DB：线程B写的DB

可能发生的意外：
1. 线程A正常先写DB后删缓存
2. 线程B是个超级耗时的操作还没写好数据库，线程C就来读了
3. ==第一个关键点：线程B还没写好，线程C来读到了旧DB==
4. 线程C读完旧DB之后，线程B终于写好了新DB，写好之后立刻删除缓存
5. ==第二个关键点：线程C还没更新缓存，线程B旧已经执行了删除操作==，没有删到缓存浪费了一次删除机会，最终导致缓存当中是旧DB值，数据库当中是新DB值，产生了不一致
缓存设置过期时间，定期更新。最终一致性，其实我们也可以接受
![[Pasted image 20240120193343.png]]

# 延迟双删模式：对缓存执行二次删除

上述线程B删除缓存之后，应当休眠xx毫秒再次删除缓存

xx毫秒怎么确定？

需要评估项目读数据业务的逻辑耗时，以确保读请求结束，写请求可删除读请求造成的缓存脏数据。如上图线程C

该策略还要考虑 redis 和数据库主从同步的耗时。最后的写数据的休眠时间：则在读数据业务逻辑的耗时的基础上，加上几百ms即可。

# 保护机制-消息队列

我们理想中的删除缓存操作并不能保证100%成功，可能由于种种原因导致缓存删除失败，这时，可能就会出现数据不一致的情况。 需提供原子性保证（操作数据库+操作缓存），即我们的重试方案。

1. 更新数据库数据
2. 如果缓存因为种种问题删除失败，就要将需要删除的key发送至消息队列
3. 自己消费消息，获得需要删除的key
4. mq的consumer方消费消息具有重试机制，因此consumer会不断重试删除操作，直到成功
![[Pasted image 20240120193725.png]]
# Canal订阅MySQL日志
上述消息队列方案有一个缺点，就是会对业务线代码造成大量的侵入。因此我们进一步考虑添加Canal
1. 启动一个订阅程序去订阅数据库的binlog，获得此次操作中被更新的数据库数据。binlog不记录MySQL读操作，只记录了MySQL的DDL和DML操作，上述缓存一致性保证策略也确实只对数据库做了修改，因此binlog完全符合我们的方案
2. 在应用程序中，另起一段非业务程序，获得这个数据库订阅程序传来的信息，将这些信息发送至消息队列
3. 尝试删除缓存操作，发现删除失败，重新从消息队列中获得该数据，重试操作。

![[Pasted image 20240120193916.png]]