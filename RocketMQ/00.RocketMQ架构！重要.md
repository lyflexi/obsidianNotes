集群架构如下，与Eureka类似
- Producer和Consumer都是作为客户端存在
- Broker实例由服务端Server来维护
![[Pasted image 20240119135447.png]]
重点是Broker，Broker里面是Topic，Topic里面是队列Queue
# Topic和队列的关系
在RocketMQ中，默认会为每个Topic在每个服务端Broker实例上创建4个队列
![[Pasted image 20240322091547.png]]

如果有两个Broker，那么默认就会有8个队列

每个Broker上的队列上的编号（queueId）都是从0开始

# 磁盘文件CommitLog

前面一直说，当消息到达RocektMQ服务端时，需要将消息存到磁盘文件

RocketMQ给这个存消息的文件起了一个高大上的名字：CommitLog

由于消息会很多，所以为了防止文件过大，CommitLog在物理磁盘文件上被分为多个磁盘文件，每个文件默认的固定大小是1G
![[Pasted image 20240322091843.png]]

消息在写入到文件时，除了包含消息本身的内容数据，也还会包含其它信息，比如
- 消息的Topic
- 消息所在队列的id，生产者发送消息时会携带这个队列id
- 消息生产者的ip
- 消息生产者的发送端口
- ...

这些数据会和消息本身按照一定的顺序同时写到CommitLog文件中
![[Pasted image 20240322091917.png]]
上图中黄色排列顺序和实际的存的内容并非实际情况，我只是举个例子


# 异步刷盘PageCache
异步刷盘的时候，为了提效都是将文件的内容先写到内核缓冲区PageCache，写到PageCache并不能保证消息一定不丢失，因为PageCache属于内存结构不稳定，如果RocketMQ服务器挂了，这部分数据是会丢失的，所以写到PageCache之后才要执行真正的刷盘==（这有点类似于Redis分布式锁在前面加一层库存的缓存，用于预扣减进一步提升分布式锁的性能）==

异步刷盘将消息真正的持久化到磁盘中，异步刷盘也有两种机制，是根据配置文件来决定的，二选一，默认是第一种
1. RocketMQ会开启一个后台线程，这个后台线程默认每隔0.5s会将消息从PageCache刷到磁盘中
2. 还有一种就是来一条消息触发一次刷盘，也是异步的
以上两种方式默认每次刷页，不足页就不刷，也就是说如果触发了刷盘，内存的页数少于4是不会去刷盘的，也就是仅靠靠这种刷盘机制，可能会有少于页的数据刷不到磁盘。为了彻底地将少于4页的数据刷到磁盘，默认每隔10s中就强制刷一次所有的数据到磁盘，所以理论上每隔1s中，磁盘的数据和内存中的数据是一样的
![[Pasted image 20240322092755.png]]

# 异步主从复制

在RocketMQ中，支持主从复制的集群模式
![[Pasted image 20240322094903.png]]

这种模式下，写消息都是写入到主节点，读消息一般也是从主节点读，但是有些情况下可能会从从节点读

从节点在启动的时候会跟主节点建立网络连接

当主节点将消息存储到CommitLog文件之后，会通过后台一个异步线程，不停地将消息发送给从节点

从节点接收到消息之后，就直接将消息存到CommitLog文件
![[Pasted image 20240322094907.png]]
# 索引文件ConsumeQueue

除了CommitLog文件之外，RocketMQ还会为每个队列创建一个磁盘文件

RocketMQ给这个文件也起了一个高大上的名字：ConsumeQueue
![[Pasted image 20240322095554.png]]

当消息被存到CommitLog之后，其实还会往这条消息所在队列的ConsumeQueue文件中插一条数据，每个队列的ConsumeQueue也是由多个文件组成，每个文件默认是存30万条数据
![[Pasted image 20240322100138.png]]
插入ConsumeQueue中的每条数据由20个字节组成，包含3部分信息
- 消息在CommitLog的起始位置（8个字节），也被称为CommitLog的偏移量offset_CommitLog
- 消息在CommitLog存储的长度（8个字节）
- 消息tag的hashCode（4个字节）
每条数据也有自己的编号，相当于自己的偏移量offset_ConsumeQueue，默认从0开始，依次递增
==可以看到ConsumeQueue中存的数据没有真实的数据内容，因此ConsumeQueue仅仅相当于索引结构，只是方便的定位到CommitLog中对应的最终数据==
![[Pasted image 20240322100711.png]]
当消费者拉取消息的时候，会告诉服务端四个比较重要的信息
- 自己需要拉取哪个Topic的消息
- 从Topic中的哪个队列（queueId）拉取
- 从队列的哪个位置（offset_ConsumeQueue）拉取消息
- 拉取多少条消息(默认32条)
![[Pasted image 20240322100739.png]]
服务端接收到消息之后，总共分为四步处理：
- 首先会找到对应的Topic
- 之后根据queueId找到对应的ConsumeQueue文件
- 然后根据offset位置，从ConsumeQueue中读取跟拉取消息条数一样条数的数据。由于ConsumeQueue每条数据都是20个字节，所以根据offset_ConsumeQueue的位置可以很快定位到应该从ConsumeQueue文件的哪个位置开始读取数据
- 最后解析每条数据，解析出offset_CommitLog，根据offset_CommitLog和消息的长度到CommitLog文件查找真正的消息内容

整个过程如下图所示：
![[Pasted image 20240322100828.png]]
由于ConsumeQueue每条数据都是20个字节，所以如果需要找第n条数据，只需要从第`n * 20`个字节的位置开始读20个字节的数据即可，这个过程是O(1)的

当从ConsumeQueue找到数据之后，解析出消息在CommitLog存储的起始位置和大小，之后就直接根据这两个信息就可以从CommitLog中找到这条消息了，这个过程也是O(1)的

所以无论CommitLog存多少消息，整个查找消息的时间复杂度都是O(1)

所以从这就可以看出，ConsumeQueue和CommitLog相互配合，就能保证快速查找到消息，消费者从而就可以快速拉取消息

# 消息压缩

另外，需要注意的是，RocketMQ在发送消息的时候，当发现消息的大小超过4k的时候，就会对消息进行压缩

这是因为如果消息过大，会对网络带宽造成压力，但如果是批量消息的话，就不会进行压缩，如下所示：
![[Pasted image 20240322090828.png]]

压缩消息除了能够减少网络带宽造成压力之外，还能够节省消息存储空间

RocketMQ在往磁盘存消息的时候，并不会去解压消息，而是直接将压缩后的消息存到磁盘

消费者拉取到的消息其实也是压缩后的消息

不过消费者在拿到消息之后会对消息进行解压缩

当我们的业务系统拿到消息的时候，其实就是解压缩后的消息
![[Pasted image 20240322090836.png]]
虽然压缩消息能够减少带宽压力和磁盘存储压力

但是由于压缩和解压缩的过程都是在客户端（生产者、消费者）完成的

所以就会导致客户端消耗更多的CPU资源，对CPU造成一定的压力

# 线程池隔离
RocketMQ在处理请求的时候，会为不同的请求分配不同的线程池进行处理

比如对于消息存储请求和拉取消息请求来说

Broker会有专门为它们分配两个不同的线程池去分别处理这些请求
![[Pasted image 20240322090639.png]]

这种让不同的业务由不同的线程池去处理的方式，能够有效地隔离不同业务逻辑之间的线程资源的影响

比如消息存储请求处理过慢并不会影响处理拉取消息请求

所以RocketMQ通过线程隔离及时可以有效地提高系统的并发性能和稳定性
# 零拷贝
零拷贝技术减少了CPU拷贝次数和上下文切换次数，实现零拷贝的有以下两种方式：
- mmap()
- sendfile()
在RocketMQ中，底层是基于mmap()来实现文件的高效读写的，sendfile()在整个过程中是无法对文件内容进行修改的，如果想修改之后再传输，可以通过mmap来修改内容之后再传输
# 顺序写

RocketMQ在存储消息时，除了使用零拷贝技术来实现文件的高效读写之外

还使用顺序写的方式提高数据写入的速度

RocketMQ会将消息按照顺序一条一条地写入文件中

这种顺序写的方式由于减少了磁头的移动和寻道时间，在大规模数据写入的场景下，使得数据写入的速度更快
# 锁优化

由于RocketMQ内部采用了很多线程异步处理机制，这就一定会产生并发情况下的线程安全问题

在这种情况下，RocketMQ进行了多方面的锁优化以提高性能和并发能力

就比如拿消息存储来说，为了保证消息是按照顺序一条一条地写入到CommitLog文件中，就需要对这个写消息的操作进行加锁

而RocketMQ默认使用ReentrantLock来加锁，并不是synchronized
![[Pasted image 20240322101542.png]]
当然除了默认情况外，RocketMQ还提供了一种基于CAS加锁的实现
![[Pasted image 20240322101548.png]]
这种实现可以在写消息压力较低的情况下使用

当然除了写消息之外，在一些其它的地方，RocketMQ也使用了基于CAS的原子操作来代替传统的锁机制

例如使用大量使用了AtomicInteger、AtomicLong等原子类来实现并发控制，避免了显式的锁竞争，提高了性能